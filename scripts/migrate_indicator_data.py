#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´¢åŠ¡æŒ‡æ ‡æ•°æ®è¡¨(indicator_data)å­—æ®µå¯¹é½è„šæœ¬
å‚è€ƒèšå®½APIè´¢åŠ¡æŒ‡æ ‡è¡¨å­—æ®µï¼Œå¯¹indicator_dataè¡¨è¿›è¡Œå­—æ®µå¯¹é½
"""

import duckdb
import pandas as pd
from typing import Dict, List, Set
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IndicatorDataMigrator:
    """è´¢åŠ¡æŒ‡æ ‡æ•°æ®è¡¨è¿ç§»å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
        
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = duckdb.connect(self.db_path)
            logger.info(f"æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")
        except Exception as e:
            logger.error(f"è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def get_current_schema(self) -> Dict[str, str]:
        """è·å–å½“å‰indicator_dataè¡¨çš„å­—æ®µç»“æ„"""
        try:
            result = self.conn.execute("PRAGMA table_info('indicator_data')").fetchall()
            schema = {}
            for row in result:
                column_name = row[1]
                column_type = row[2]
                schema[column_name] = column_type
            logger.info(f"å½“å‰indicator_dataè¡¨æœ‰ {len(schema)} ä¸ªå­—æ®µ")
            return schema
        except Exception as e:
            logger.error(f"è·å–å½“å‰è¡¨ç»“æ„å¤±è´¥: {e}")
            return {}
    
    def get_target_schema(self) -> Dict[str, str]:
        """å®šä¹‰ç›®æ ‡å­—æ®µç»“æ„ - ä¸¥æ ¼åŸºäºèšå®½API indicatorè´¢åŠ¡æŒ‡æ ‡è¡¨"""
        # ä¸¥æ ¼æŒ‰ç…§èšå®½å®˜æ–¹APIæ–‡æ¡£ indicatorè¡¨å­—æ®µå®šä¹‰
        # å‚è€ƒ: https://www.joinquant.com/help/api/doc?name=JQDatadoc&id=9885
        target_schema = {
            # åŸºç¡€å­—æ®µ
            'code': 'VARCHAR',  # è‚¡ç¥¨ä»£ç  å¸¦åç¼€.XSHE/.XSHG
            'pubDate': 'DATE',  # å…¬å¸å‘å¸ƒè´¢æŠ¥æ—¥æœŸ
            'statDate': 'DATE',  # è´¢æŠ¥ç»Ÿè®¡çš„å­£åº¦çš„æœ€åä¸€å¤©
            
            # ç›ˆåˆ©èƒ½åŠ›æŒ‡æ ‡
            'eps': 'DOUBLE',  # æ¯è‚¡æ”¶ç›ŠEPS(å…ƒ)
            'adjusted_profit': 'DOUBLE',  # æ‰£é™¤éç»å¸¸æŸç›Šåçš„å‡€åˆ©æ¶¦(å…ƒ)
            'operating_profit': 'DOUBLE',  # ç»è¥æ´»åŠ¨å‡€æ”¶ç›Š(å…ƒ)
            'value_change_profit': 'DOUBLE',  # ä»·å€¼å˜åŠ¨å‡€æ”¶ç›Š(å…ƒ)
            'roe': 'DOUBLE',  # å‡€èµ„äº§æ”¶ç›Šç‡ROE(%)
            'inc_return': 'DOUBLE',  # å‡€èµ„äº§æ”¶ç›Šç‡(æ‰£é™¤éç»å¸¸æŸç›Š)(%)
            'roa': 'DOUBLE',  # æ€»èµ„äº§å‡€åˆ©ç‡ROA(%)
            'net_profit_margin': 'DOUBLE',  # é”€å”®å‡€åˆ©ç‡(%)
            'gross_profit_margin': 'DOUBLE',  # é”€å”®æ¯›åˆ©ç‡(%)
            
            # æˆæœ¬è´¹ç”¨æŒ‡æ ‡
            'expense_to_total_revenue': 'DOUBLE',  # è¥ä¸šæ€»æˆæœ¬/è¥ä¸šæ€»æ”¶å…¥(%)
            'operation_profit_to_total_revenue': 'DOUBLE',  # è¥ä¸šåˆ©æ¶¦/è¥ä¸šæ€»æ”¶å…¥(%)
            'net_profit_to_total_revenue': 'DOUBLE',  # å‡€åˆ©æ¶¦/è¥ä¸šæ€»æ”¶å…¥(%)
            'operating_expense_to_total_revenue': 'DOUBLE',  # è¥ä¸šè´¹ç”¨/è¥ä¸šæ€»æ”¶å…¥(%)
            'ga_expense_to_total_revenue': 'DOUBLE',  # ç®¡ç†è´¹ç”¨/è¥ä¸šæ€»æ”¶å…¥(%)
            'financing_expense_to_total_revenue': 'DOUBLE',  # è´¢åŠ¡è´¹ç”¨/è¥ä¸šæ€»æ”¶å…¥(%)
            
            # ç›ˆåˆ©è´¨é‡æŒ‡æ ‡
            'operating_profit_to_profit': 'DOUBLE',  # ç»è¥æ´»åŠ¨å‡€æ”¶ç›Š/åˆ©æ¶¦æ€»é¢(%)
            'invesment_profit_to_profit': 'DOUBLE',  # ä»·å€¼å˜åŠ¨å‡€æ”¶ç›Š/åˆ©æ¶¦æ€»é¢(%)
            'adjusted_profit_to_profit': 'DOUBLE',  # æ‰£é™¤éç»å¸¸æŸç›Šåçš„å‡€åˆ©æ¶¦/å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦(%)
            
            # ç°é‡‘æµæŒ‡æ ‡
            'goods_sale_and_service_to_revenue': 'DOUBLE',  # é”€å”®å•†å“æä¾›åŠ³åŠ¡æ”¶åˆ°çš„ç°é‡‘/è¥ä¸šæ”¶å…¥(%)
            'ocf_to_revenue': 'DOUBLE',  # ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢/è¥ä¸šæ”¶å…¥(%)
            'ocf_to_operating_profit': 'DOUBLE',  # ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢/ç»è¥æ´»åŠ¨å‡€æ”¶ç›Š(%)
            
            # æˆé•¿èƒ½åŠ›æŒ‡æ ‡
            'inc_total_revenue_year_on_year': 'DOUBLE',  # è¥ä¸šæ€»æ”¶å…¥åŒæ¯”å¢é•¿ç‡(%)
            'inc_total_revenue_annual': 'DOUBLE',  # è¥ä¸šæ€»æ”¶å…¥ç¯æ¯”å¢é•¿ç‡(%)
            'inc_revenue_year_on_year': 'DOUBLE',  # è¥ä¸šæ”¶å…¥åŒæ¯”å¢é•¿ç‡(%)
        }
        
        logger.info(f"ç›®æ ‡schemaå®šä¹‰äº† {len(target_schema)} ä¸ªå­—æ®µ")
        return target_schema
    
    def analyze_schema_differences(self, current: Dict[str, str], target: Dict[str, str]) -> Dict[str, List[str]]:
        """åˆ†æå½“å‰schemaä¸ç›®æ ‡schemaçš„å·®å¼‚"""
        current_fields = set(current.keys())
        target_fields = set(target.keys())
        
        missing_fields = target_fields - current_fields
        redundant_fields = current_fields - target_fields
        common_fields = current_fields & target_fields
        
        differences = {
            'missing': list(missing_fields),
            'redundant': list(redundant_fields),
            'common': list(common_fields)
        }
        
        logger.info(f"å­—æ®µåˆ†æç»“æœ:")
        logger.info(f"  ç¼ºå¤±å­—æ®µ: {len(missing_fields)} ä¸ª")
        logger.info(f"  å†—ä½™å­—æ®µ: {len(redundant_fields)} ä¸ª")
        logger.info(f"  å…±åŒå­—æ®µ: {len(common_fields)} ä¸ª")
        
        if missing_fields:
            logger.info(f"  ç¼ºå¤±å­—æ®µåˆ—è¡¨: {sorted(missing_fields)}")
        if redundant_fields:
            logger.info(f"  å†—ä½™å­—æ®µåˆ—è¡¨: {sorted(redundant_fields)}")
            
        return differences
    
    def add_missing_columns(self, missing_fields: List[str], target_schema: Dict[str, str]):
        """æ·»åŠ ç¼ºå¤±çš„å­—æ®µ"""
        if not missing_fields:
            logger.info("æ²¡æœ‰éœ€è¦æ·»åŠ çš„å­—æ®µ")
            return
            
        logger.info(f"å¼€å§‹æ·»åŠ  {len(missing_fields)} ä¸ªç¼ºå¤±å­—æ®µ...")
        
        for field in missing_fields:
            try:
                field_type = target_schema[field]
                sql = f"ALTER TABLE indicator_data ADD COLUMN {field} {field_type}"
                self.conn.execute(sql)
                logger.info(f"  âœ“ æ·»åŠ å­—æ®µ: {field} ({field_type})")
            except Exception as e:
                logger.error(f"  âœ— æ·»åŠ å­—æ®µ {field} å¤±è´¥: {e}")
    
    def create_table_if_not_exists(self, target_schema: Dict[str, str]):
        """å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»º"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            result = self.conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name='indicator_data'"
            ).fetchall()
            
            if result:
                logger.info("indicator_dataè¡¨å·²å­˜åœ¨")
                return
                
            logger.info("indicator_dataè¡¨ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º...")
            
            # æ„å»ºCREATE TABLEè¯­å¥
            columns_def = []
            for field, field_type in target_schema.items():
                columns_def.append(f"{field} {field_type}")
            
            columns_str = ',\n    '.join(columns_def)
            
            create_sql = f"""
            CREATE TABLE indicator_data (
                {columns_str},
                PRIMARY KEY (code, day)
            )
            """
            
            self.conn.execute(create_sql)
            logger.info("âœ“ indicator_dataè¡¨åˆ›å»ºæˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆ›å»ºè¡¨å¤±è´¥: {e}")
            raise
    
    def backup_table(self):
        """å¤‡ä»½åŸè¡¨"""
        try:
            backup_name = f"indicator_data_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            sql = f"CREATE TABLE {backup_name} AS SELECT * FROM indicator_data"
            self.conn.execute(sql)
            
            # è·å–å¤‡ä»½è¡¨è®°å½•æ•°
            count_result = self.conn.execute(f"SELECT COUNT(*) FROM {backup_name}").fetchone()
            record_count = count_result[0] if count_result else 0
            
            logger.info(f"âœ“ è¡¨å¤‡ä»½å®Œæˆ: {backup_name} (å…± {record_count} æ¡è®°å½•)")
            return backup_name
        except Exception as e:
            logger.error(f"å¤‡ä»½è¡¨å¤±è´¥: {e}")
            raise
    
    def verify_migration(self, target_schema: Dict[str, str]) -> bool:
        """éªŒè¯è¿ç§»ç»“æœ"""
        try:
            current_schema = self.get_current_schema()
            target_fields = set(target_schema.keys())
            current_fields = set(current_schema.keys())
            
            missing_fields = target_fields - current_fields
            
            if missing_fields:
                logger.error(f"éªŒè¯å¤±è´¥: ä»æœ‰ {len(missing_fields)} ä¸ªå­—æ®µç¼ºå¤±: {sorted(missing_fields)}")
                return False
            else:
                logger.info("âœ“ éªŒè¯æˆåŠŸ: æ‰€æœ‰ç›®æ ‡å­—æ®µéƒ½å·²å­˜åœ¨")
                return True
                
        except Exception as e:
            logger.error(f"éªŒè¯è¿ç§»ç»“æœå¤±è´¥: {e}")
            return False
    
    def migrate(self):
        """æ‰§è¡Œå®Œæ•´çš„è¿ç§»æµç¨‹"""
        try:
            logger.info("å¼€å§‹indicator_dataè¡¨å­—æ®µå¯¹é½è¿ç§»...")
            
            # è¿æ¥æ•°æ®åº“
            self.connect()
            
            # è·å–ç›®æ ‡schema
            target_schema = self.get_target_schema()
            
            # åˆ›å»ºè¡¨(å¦‚æœä¸å­˜åœ¨)
            self.create_table_if_not_exists(target_schema)
            
            # å¤‡ä»½åŸè¡¨
            backup_name = self.backup_table()
            
            # è·å–å½“å‰schema
            current_schema = self.get_current_schema()
            
            # åˆ†æå·®å¼‚
            differences = self.analyze_schema_differences(current_schema, target_schema)
            
            # æ·»åŠ ç¼ºå¤±å­—æ®µ
            self.add_missing_columns(differences['missing'], target_schema)
            
            # éªŒè¯è¿ç§»ç»“æœ
            if self.verify_migration(target_schema):
                logger.info("ğŸ‰ indicator_dataè¡¨å­—æ®µå¯¹é½è¿ç§»å®Œæˆ!")
                logger.info(f"ğŸ“Š è¿ç§»ç»Ÿè®¡:")
                logger.info(f"  - ç›®æ ‡å­—æ®µæ€»æ•°: {len(target_schema)}")
                logger.info(f"  - æ–°å¢å­—æ®µæ•°: {len(differences['missing'])}")
                logger.info(f"  - ä¿ç•™å­—æ®µæ•°: {len(differences['common'])}")
                logger.info(f"  - å¤‡ä»½è¡¨å: {backup_name}")
            else:
                logger.error("âŒ è¿ç§»éªŒè¯å¤±è´¥")
                
        except Exception as e:
            logger.error(f"è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        finally:
            if self.conn:
                self.conn.close()
                logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®åº“è·¯å¾„
    db_path = "data/stock_data_new.duckdb"
    
    # åˆ›å»ºè¿ç§»å™¨å¹¶æ‰§è¡Œè¿ç§»
    migrator = IndicatorDataMigrator(db_path)
    migrator.migrate()

if __name__ == "__main__":
    main()